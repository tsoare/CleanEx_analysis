{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CleanEx: Model selection\n",
    "In this notebook, I am using recursive feature elimination to perform model selection in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load entire classification dataset (n=3400 genes)\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sample top and bottom 10% and classify\n",
    "d = pd.read_csv(open(\"input_data_top-bottom_classify.csv\"), index_col=0)\n",
    "# remove unncessary columns\n",
    "d = d.drop(['liver.ratio', 'log2.liver.ratio', 'decile'], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly split data into training and test sets (70:30 split), normalizing the predictors\n",
    "# set random number seed to be replicable\n",
    "seed = np.random.seed(seed=6892)\n",
    "# randomly assign the order\n",
    "d['random_order'] = np.random.permutation(len(d))\n",
    "d = d.sort_values(by='random_order', ascending=True)\n",
    "d = d.drop('random_order', 1)\n",
    "\n",
    "# separate outcome from predictors\n",
    "X = d.drop('highly.liver.expressed', 1)\n",
    "y = d['highly.liver.expressed']\n",
    "\n",
    "# normalize predictors\n",
    "X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "# split:\n",
    "X_train = X[:-1000]\n",
    "X_test  = X[-1000:]\n",
    "y_train = y[:-1000]\n",
    "y_test  = y[-1000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saturated model\n",
    "Here I am running the saturated model to ensure that the features can reliably classify the top 10% and bottom 10% of genes in the training data, using 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55994403656 0.0179865029108\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression and print the mean and SD of the accuracy across all 5-folds\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "my_logreg = LogisticRegression()\n",
    "scores = cross_val_score(my_logreg, X=X_train, y=y_train, cv=5)\n",
    "print np.mean(scores), np.std(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.553655605554 0.015572048752\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines and print the mean and SD of the accuracy across all 5-folds\n",
    "from sklearn.svm import SVC\n",
    "my_svc = SVC(kernel=\"linear\")\n",
    "scores = cross_val_score(my_svc, X=X_train, y=y_train, cv=5)\n",
    "print np.mean(scores), np.std(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.563711481277 0.0203670459472\n"
     ]
    }
   ],
   "source": [
    "# Random Forests and print the mean and SD of the accuracy across all 5-folds\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "my_rfc = RandomForestClassifier(random_state=2)\n",
    "scores = cross_val_score(my_rfc, X=X_train, y=y_train, cv=5)\n",
    "print np.mean(scores), np.std(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "Here I am using recursive feature elimination to eliminate superfluous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0         1         2\n",
      "0  200  0.546113  0.014986\n",
      "1   20  0.551566  0.028200\n",
      "2   10  0.529765  0.031236\n"
     ]
    }
   ],
   "source": [
    "# recursive feature elimination for logistic regression\n",
    "from sklearn.feature_selection import RFE\n",
    "selector = RFE(my_logreg, 200, step=1000)\n",
    "selector = selector.fit(X=X_train, y=y_train)\n",
    "#print selector.ranking_\n",
    "\n",
    "# scores of log reg after RFE:\n",
    "r = []\n",
    "for i in [200, 20, 10]:\n",
    "    selector = RFE(my_logreg, i, step=1000)\n",
    "    scores = cross_val_score(selector, X=X_train, y=y_train, cv=5)\n",
    "    r.append([i, np.mean(scores), np.std(scores)])\n",
    "print pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0         1         2\n",
      "0  200  0.541074  0.014655\n",
      "1   20  0.542744  0.013471\n",
      "2   10  0.525141  0.010749\n"
     ]
    }
   ],
   "source": [
    "# recursive feature elimination for support vector machines\n",
    "r = []\n",
    "for i in [200, 20, 10]:\n",
    "    selector = RFE(my_svc, i, step=1000)\n",
    "    scores = cross_val_score(selector, X=X_train, y=y_train, cv=5)\n",
    "    r.append([i, np.mean(scores), np.std(scores)])\n",
    "print pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0         1         2\n",
      "0  200  0.567051  0.010256\n",
      "1   20  0.576267  0.019045\n",
      "2   10  0.557012  0.017123\n"
     ]
    }
   ],
   "source": [
    "# recursive feature elimination for random forests\n",
    "from sklearn.feature_selection import RFE\n",
    "r = []\n",
    "for i in [200, 20, 10]:\n",
    "    selector = RFE(my_rfc, i, step=1000)\n",
    "    scores = cross_val_score(selector, X=X_train, y=y_train, cv=5)\n",
    "    r.append([i, np.mean(scores), np.std(scores)])\n",
    "print pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AC' 'AGG' 'AGGG' 'CC' 'CG' 'CGAA' 'CT' 'CTG' 'TCA' 'TCT' 'TGG' 'MA0528.1'\n",
      " 'MA0528.1_max.score' 'MA0685.1_max.score' 'MA0056.1_max.start.pos'\n",
      " 'MA0149.1_max.start.pos' 'MA0478.1_max.start.pos' 'MA0508.1_max.start.pos'\n",
      " 'MA0597.1_max.start.pos' 'MA0759.1_max.start.pos']\n"
     ]
    }
   ],
   "source": [
    "# print 20 most important features selected in random forests model\n",
    "selector = RFE(my_rfc, 20, step=1000)\n",
    "selector = selector.fit(X=X_train, y=y_train)\n",
    "\n",
    "headers = np.array(list(X_train))\n",
    "print headers[selector.support_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 342  854  897 1707 2048 2050 2389 2560 4438 4693 4949 5593 5979 6040 6254\n",
      " 6304 6334 6353 6369 6492]\n"
     ]
    }
   ],
   "source": [
    "# print vector of column indices of 20 most important features\n",
    "s = selector.ranking_\n",
    "i, = np.where( s==1 )\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         0          1\n",
      "4                       CG  0.0734069\n",
      "12      MA0528.1_max.score  0.0704381\n",
      "3                       CC  0.0653915\n",
      "14  MA0056.1_max.start.pos  0.0577468\n",
      "9                      TCT  0.0574966\n",
      "6                       CT  0.0567954\n",
      "8                      TCA  0.0564194\n",
      "7                      CTG  0.0557282\n",
      "10                     TGG  0.0554651\n",
      "0                       AC  0.0550295\n",
      "15  MA0149.1_max.start.pos  0.0547808\n",
      "1                      AGG  0.0540682\n",
      "2                     AGGG  0.0489609\n",
      "17  MA0508.1_max.start.pos  0.0456348\n",
      "11                MA0528.1  0.0451924\n",
      "13      MA0685.1_max.score  0.0365594\n",
      "16  MA0478.1_max.start.pos  0.0360116\n",
      "5                     CGAA  0.0281987\n",
      "18  MA0597.1_max.start.pos  0.0255982\n",
      "19  MA0759.1_max.start.pos  0.0210776\n"
     ]
    }
   ],
   "source": [
    "# perform a random forests on these features\n",
    "X_select = X_train.loc[:, headers[selector.support_]]\n",
    "\n",
    "my_rfc = RandomForestClassifier(random_state=2)\n",
    "my_rfc.fit(X=X_select, y=y_train)\n",
    "\n",
    "# get feature importances\n",
    "a = [headers[selector.support_]]\n",
    "a.append(my_rfc.feature_importances_)\n",
    "a = pd.DataFrame(a).transpose()\n",
    "a.to_csv(\"RF_top20features.csv\")\n",
    "print a.sort_values(by=1, ascending=False)\n",
    "\n",
    "# rank features by importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model accuracy in test data\n",
    "Here I am carrying forward the 20 features identified in the random forest model and assessing the prediction accuracy of the 3 different models on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55700000000000005"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict test data with random forests and check accuracy\n",
    "X_test_select = X_test.loc[:, headers[selector.support_]]\n",
    "my_rfc.score(X=X_test_select, \n",
    "                y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59099999999999997"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict test data with support vector machines and check accuracy\n",
    "my_svc = SVC(kernel=\"linear\")\n",
    "my_svc.fit(X=X_select, y=y_train)\n",
    "my_svc.score(X=X_test_select, y=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60199999999999998"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict test data with logistic regression and check accuracy\n",
    "my_logreg = LogisticRegression()\n",
    "my_logreg.fit(X=X_select, y=y_train)\n",
    "\n",
    "\n",
    "my_logreg.score(X=X_test_select, \n",
    "                y=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         0          1\n",
      "0                       AC   0.170571\n",
      "1                      AGG -0.0465778\n",
      "2                     AGGG  -0.107238\n",
      "3                       CC   -0.13875\n",
      "4                       CG   0.332335\n",
      "5                     CGAA    0.10882\n",
      "6                       CT  0.0419476\n",
      "7                      CTG -0.0752032\n",
      "8                      TCA   0.134025\n",
      "9                      TCT -0.0747132\n",
      "10                     TGG  0.0944133\n",
      "11                MA0528.1 -0.0535979\n",
      "12      MA0528.1_max.score   -0.14182\n",
      "13      MA0685.1_max.score   0.173017\n",
      "14  MA0056.1_max.start.pos -0.0435735\n",
      "15  MA0149.1_max.start.pos  -0.147209\n",
      "16  MA0478.1_max.start.pos   -0.15848\n",
      "17  MA0508.1_max.start.pos  0.0939448\n",
      "18  MA0597.1_max.start.pos  0.0153757\n",
      "19  MA0759.1_max.start.pos   0.228075\n"
     ]
    }
   ],
   "source": [
    "# print coefficients of logistic regression (obtain direction of effect)\n",
    "b = [np.array(list(X_select))]\n",
    "b.append(my_logreg.coef_.reshape(20,))\n",
    "b = pd.DataFrame(b).transpose()\n",
    "b.to_csv(\"top20features_LogReg_coefficients.csv\")\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.601953631258\n"
     ]
    }
   ],
   "source": [
    "# compute AUC of logistic regression model\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "print roc_auc_score(y_test, \n",
    "                    my_logreg.predict(X_test_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.        ,  0.38645418,  1.        ]),\n",
       " array([ 0.        ,  0.59036145,  1.        ]),\n",
       " array([2, 1, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot ROC curve\n",
    "roc_curve(y_test, my_logreg.predict(X_test_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[308, 194],\n",
       "       [204, 294]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate confusion matrix of model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred=my_logreg.predict(X_test_select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.61      0.61       502\n",
      "          1       0.60      0.59      0.60       498\n",
      "\n",
      "avg / total       0.60      0.60      0.60      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# return classification report\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, y_pred=my_logreg.predict(X_test_select))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the logistic regression model, precision is 0.60, recall is 0.59, and F1 is 0.60.  Thus the error is balanced equally among both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save train and test data for web app input\n",
    "X_select.to_csv(\"top20features_X_train.csv\")\n",
    "X_test_select.to_csv(\"top20features_X_test.csv\")\n",
    "\n",
    "y_train.to_csv(\"y_train.csv\")\n",
    "y_test.to_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59299999999999997"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict test data with logistic regression using subset of features: 11 n-gram features\n",
    "# subset to n-grams:\n",
    "X_select2 = X_select.iloc[:, :11]\n",
    "X_test_select2 = X_test_select.iloc[:, :11]\n",
    "\n",
    "my_logreg2 = LogisticRegression()\n",
    "my_logreg2.fit(X=X_select2, y=y_train)\n",
    "\n",
    "# predict:\n",
    "my_logreg2.score(X=X_test_select2, \n",
    "                y=y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final model and other inputs for web app\n",
    "Here I am saving the final logistic regression model as well as the means and SDs of the features in the training data for the calculations in the web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save this basic model for webapp\n",
    "import pickle\n",
    "\n",
    "filename = 'logreg_model_basic.sav'\n",
    "pickle.dump(my_logreg2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# also save mean and SD of all X_select rows to normalize input features\n",
    "means = X_select2.mean(axis=0)\n",
    "SDs = X_select2.std(axis=0)\n",
    "\n",
    "pickle.dump(means, open(\"feature_means.sav\", 'wb'))\n",
    "pickle.dump(SDs, open(\"feature_SDs.sav\", 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
